[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "PaperBreak",
    "section": "",
    "text": "Breaking the Softmax Bottleneck\n\n\n\n\n\n\ntheory\n\n\nlanguage modeling\n\n\n\n\n\n\n\n\n\nFeb 18, 2025\n\n\nC.H.\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/softmax-bottleneck/index.html",
    "href": "posts/softmax-bottleneck/index.html",
    "title": "Breaking the Softmax Bottleneck",
    "section": "",
    "text": "A breakdown of the paper Breaking the Softmax Bottleneck: A High-Rank RNN Language Model"
  },
  {
    "objectID": "posts/softmax-bottleneck/index.html#definitions",
    "href": "posts/softmax-bottleneck/index.html#definitions",
    "title": "Breaking the Softmax Bottleneck",
    "section": "Definitions",
    "text": "Definitions\n\nLet \\(A = \\log \\mathbb{P}^*(X \\vert C)\\)\n\nNote: here we use the notation \\(\\mathbb{P}^*\\) to denote the true distribution. In practice, we donâ€™t know what the true distribution is - we are trying to solve for it.\n\nLet \\(F(A)\\) be the set of row-wise shifts of the matrix \\(A\\):\n\n\\[\n\\begin{align}\nF(A) & = \\{ A + C \\space \\vert \\space C_{i,:} = c, c \\in \\mathbb{R}  \\} & (1) \\\\\n& = \\{ A + \\Lambda J_{m,n} \\space \\vert \\space \\Lambda \\text{ is diagonal}, \\Lambda \\in \\mathbb{R}^{n \\times n} \\} & (2)\n\\end{align}\n\\]\nAt first glance, the set \\(F(A)\\) might seem very unnatural - but it essentially just represents the idea that Softmax is shift-invariant: \\[\n\\text{Softmax}(\\vec x) = \\frac{\\exp(\\vec x)}{\\sum_j \\exp(x_j)}\n\\] If we shift each element of \\(\\vec x\\) by the same value \\(c\\), formally \\(\\vec x + \\vec c\\) where \\(\\vec c = [c, c, \\dots, c]\\), then \\(\\text{Softmax}(\\vec x + \\vec c) = \\text{Softmax}(\\vec x)\\). \\(F(A)\\) is just the matrix form of this idea.\nNote: I wrote two equivalent definitions for \\(A\\) for (1) and (2) - definition (1) is easier to understand, and definition (2) is more useful for one of our proofs later."
  },
  {
    "objectID": "posts/softmax-bottleneck/index.html#properties",
    "href": "posts/softmax-bottleneck/index.html#properties",
    "title": "Breaking the Softmax Bottleneck",
    "section": "Properties",
    "text": "Properties\n\nFor any matrix \\(A'\\), \\(A' \\in F(A) \\iff \\text{Softmax}(A') = \\mathbb{P}^*(X \\vert C)\\)\n\nThe set \\(F(A)\\) defines the set of all logits that correspond to the true probability distribution $ ^* $. This means our logits \\(H_\\theta W_\\theta^\\top\\) have to be in the set \\(F(A)\\) for our NN to be able to approximate the true distribution \\(\\mathbb{P}^*\\).\n\\(\\implies\\)\n\\[\n\\begin{aligned}\n    &  A' &\\in& F(A) & (1) \\\\\n    \\implies & A' &=& A + C & (2) \\\\\n    \\implies & \\text{Softmax}(A') &=& \\text{Softmax}(A + C) & (3) \\\\\n    & &=& \\text{Softmax}(A) & (4) \\\\\n    & &=& \\text{Softmax}(\\log \\mathbb{P}^*(X \\vert C)) & (5) \\\\\n    & &=& \\mathbb{P}^*(X \\vert C) \\quad \\blacksquare & (6) &\n\\end{aligned}\n\\]\nNote: Softmax is shift-invariant (line 3)\n\\(\\impliedby\\)\n\\[\n\\begin{aligned}\n& \\text{Softmax}(A') &=& \\mathbb{P}^*(X \\vert C) & (1) \\\\\n\\implies & \\log \\text{Softmax}(A') &=& \\log \\mathbb{P}^*(X \\vert C) & (2) \\\\\n\\implies & \\log \\text{Softmax}(A') &=& A & (3) \\\\\n\\implies & A' + \\log \\Sigma \\exp(A') &=& A & (4) \\\\\n\\implies & A' &=& A - \\log \\Sigma \\exp(A') & (5) \\\\\n& &\\in& \\space F(A)  \\quad \\blacksquare & (6) &\n\\end{aligned}\n\\]\nNote: Line (4) above is abuse of notation - Softmax of a matrix is generally understood to be applied row-wise.\n\n\\(\\forall A_1 \\neq A_2 \\in F(A), \\vert \\text{rank}(A_1) - \\text{rank}(A_2) \\vert \\leq 1\\)\n\nThis is saying that all matrices that belong to the set \\(F(A)\\) have a similar rank. This is not surprising - every matrix in \\(F(A)\\) is simply row-wise shifted by some constant factor.\n\\(\\implies\\)\n\\[\n\\begin{aligned}\n    & \\text{Given } A_1 \\neq A_2 \\in F(A) & (1) \\\\\n    & A_1 = A + \\Lambda_1 \\mathbb{J}_{m,n} & (2) \\\\\n    & A_2 = A + \\Lambda_2 \\mathbb{J}_{m,n} & (3) \\\\\n    & \\implies A_1 = A_2 + (\\Lambda_1 - \\Lambda_2)\\mathbb{J}_{m,n} & (4) \\\\\n    & \\implies \\text{rank}(A_1) = \\text{rank}(A_2 + (\\Lambda_1 - \\Lambda_2)\\mathbb{J}_{m,n}) & (5) \\\\\n    & \\phantom{\\implies} \\phantom{\\text{rank}(A_1)} \\leq \\text{rank}(A_2) + \\text{rank}((\\Lambda_1 - \\Lambda_2)\\mathbb{J}_{m,n}) & (6) \\\\\n    & \\phantom{\\implies} \\phantom{\\text{rank}(A_1)} \\leq \\text{rank}(A_2) + \\min(\\text{rank}(\\Lambda_1 - \\Lambda_2),\\text{rank}(\\mathbb{J}_{m,n})) & (7) \\\\\n    & \\phantom{\\implies} \\phantom{\\text{rank}(A_1)} \\leq \\text{rank}(A_2) + 1 & (8) \\\\\n    & \\text{similarly, we can find that } \\text{rank}({A_2}) \\leq \\text{rank}(A_1) - 1 & (9) \\\\\n    & \\implies \\vert \\text{rank}(A_1) - \\text{rank}(A_2) \\vert \\leq 1 \\quad \\blacksquare & (10) \\\\\n\\end{aligned}\n\\]"
  }
]