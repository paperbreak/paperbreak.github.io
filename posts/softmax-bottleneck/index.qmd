---
title:  "Breaking the Softmax Bottleneck"
author: "C.H."
date:   2025-02-18
categories: [theory, language modeling]
---

A breakdown of the paper [Breaking the Softmax Bottleneck: A High-Rank RNN Language Model](https://arxiv.org/abs/1711.03953)

# Background

When doing Language Modeling, we try and train a model to learn the conditional next-token distribution $\mathbb{P}(X \vert C)$, where $X= \lbrace x_1, \dots , x_m \rbrace$ is our vocabulary (set of tokens) and $C = \lbrace c_1, \dots, c_n \rbrace$ is the set of contexts. What the set $C$ looks like is based on the type of language modeling we want to perform, for example left-to-right or masked language modeling. In general, the context is just the tokens surrounding a particular token.

We can visualize this distribution as a matrix, with the number of rows being equal to the number of unique tokens $m$, and the number of columns being equal to the number of unique contexts $n$. Each entry at position $(i, j)$ represents the probability of token $x_i$ appearing in context $c_j$.

The way we train our NN to fit this distribution is in the following way:

$$
\begin{aligned}
\log \mathbb{P}(X \vert C) & \approx \log \mathbb{P}_\theta(X \vert C) \\
& = \log \text{Softmax} ( H_\theta W_\theta^\top ) \\
\end{aligned}
$$

Where $W_\theta$ is our embedding weights, and $H_\theta$ is our context vectors (output of the final layer of our NN *before* the output embedding).
Essentially, this boils down to a matrix factorization problem - we are trying to approximate the theoretical distribution on the left as a product of two
matricies on the right.

Looking deeper, when we initialize the embedding weights $W_\theta$, we have to also pick their dimension size $d$. 
Therefore $W_\theta \in \mathbb{R}^{m \times d}$ and $H_\theta \in \mathbb{R}^{n \times d}$. This means we are performing a 
[Rank Factorization](https://en.wikipedia.org/wiki/Rank_factorization) of $\mathbb{P}(X \vert C)$, and $\text{rank}(\mathbb{P}(X \vert C)) = d$.

But this is now a bit concerning - in practice, we usually set $d$ to be a relatively low number, for example 786 for BERT.
This begins to imply the existance of the **Softmax Bottleneck**: if we set our embedding dimension $d$ too low, we will not
be able to accurately approximate the true distribution $\mathbb{P}(X \vert C)$.

# Formal Justification

## Definitions

1) Let $A = \log \mathbb{P}^*(X \vert C)$

Note: here we use the notation $\mathbb{P}^*$ to denote the true distribution. In practice, we don't know what the true distribution is - we are trying to solve for it.

2) Let $F(A)$ be the set of *row-wise* shifts of the matrix $A$:

$$
\begin{align}
F(A) & = \{ A + C \space \vert \space C_{i,:} = c, c \in \mathbb{R}  \} & (1) \\
& = \{ A + \Lambda J_{m,n} \space \vert \space \Lambda \text{ is diagonal}, \Lambda \in \mathbb{R}^{n \times n} \} & (2)
\end{align}
$$

At first glance, the set $F(A)$ might seem very unnatural - but it essentially just represents the idea that Softmax is shift-invariant:
$$
\text{Softmax}(\vec x) = \frac{\exp(\vec x)}{\sum_j \exp(x_j)}
$$
If we shift each element of $\vec x$ by the same value $c$, formally $\vec x + \vec c$ where $\vec c = [c, c, \dots, c]$, then $\text{Softmax}(\vec x + \vec c) = \text{Softmax}(\vec x)$.
$F(A)$ is just the matrix form of this idea.

Note: I wrote two equivalent definitions for $A$ for (1) and (2) - definition (1) is easier to understand,
and definition (2) is more useful for one of our proofs later.

## Properties

1) For any matrix $A'$, $A' \in F(A) \iff \text{Softmax}(A') = \mathbb{P}^*(X \vert C)$

The set $F(A)$ defines the set of all logits that correspond to the true probability distribution $\mathbb{P}^*$.  This means our logits $H_\theta W_\theta^\top$ have to be in the set $F(A)$ for our NN to be able to approximate the true distribution $\mathbb{P}^*$.


$\implies$

$$
\begin{aligned}
    &  A' &\in& F(A) & (1) \\
    \implies & A' &=& A + C & (2) \\
    \implies & \text{Softmax}(A') &=& \text{Softmax}(A + C) & (3) \\
    & &=& \text{Softmax}(A) & (4) \\
    & &=& \text{Softmax}(\log \mathbb{P}^*(X \vert C)) & (5) \\
    & &=& \mathbb{P}^*(X \vert C) \quad \blacksquare & (6) &
\end{aligned}
$$

Note: Softmax is shift-invariant (line 3)


$\impliedby$

$$
\begin{aligned}
& \text{Softmax}(A') &=& \mathbb{P}^*(X \vert C) & (1) \\
\implies & \log \text{Softmax}(A') &=& \log \mathbb{P}^*(X \vert C) & (2) \\
\implies & \log \text{Softmax}(A') &=& A & (3) \\
\implies & A' + \log \Sigma \exp(A') &=& A & (4) \\
\implies & A' &=& A - \log \Sigma \exp(A') & (5) \\
& &\in& \space F(A)  \quad \blacksquare & (6) &
\end{aligned}
$$

Note: Line (4) above is abuse of notation - Softmax of a matrix is generally understood to be applied *row-wise*.

2) $\forall A_1 \neq A_2 \in F(A), \vert \text{rank}(A_1) - \text{rank}(A_2) \vert \leq 1$

This is saying that all matrices that belong to the set $F(A)$ have a similar rank. This is not surprising - every matrix in $F(A)$ is simply *row-wise* shifted by some constant factor.

$\implies$

$$
\begin{aligned}
    & \text{Given } A_1 \neq A_2 \in F(A) & (1) \\
    & A_1 = A + \Lambda_1 \mathbb{J}_{m,n} & (2) \\
    & A_2 = A + \Lambda_2 \mathbb{J}_{m,n} & (3) \\
    & \implies A_1 = A_2 + (\Lambda_1 - \Lambda_2)\mathbb{J}_{m,n} & (4) \\
    & \implies \text{rank}(A_1) = \text{rank}(A_2 + (\Lambda_1 - \Lambda_2)\mathbb{J}_{m,n}) & (5) \\
    & \phantom{\implies} \phantom{\text{rank}(A_1)} \leq \text{rank}(A_2) + \text{rank}((\Lambda_1 - \Lambda_2)\mathbb{J}_{m,n}) & (6) \\
    & \phantom{\implies} \phantom{\text{rank}(A_1)} \leq \text{rank}(A_2) + \min(\text{rank}(\Lambda_1 - \Lambda_2),\text{rank}(\mathbb{J}_{m,n})) & (7) \\
    & \phantom{\implies} \phantom{\text{rank}(A_1)} \leq \text{rank}(A_2) + 1 & (8) \\
    & \text{similarly, we can find that } \text{rank}({A_2}) \leq \text{rank}(A_1) - 1 & (9) \\
    & \implies \vert \text{rank}(A_1) - \text{rank}(A_2) \vert \leq 1 \quad \blacksquare & (10) \\
\end{aligned}
$$
