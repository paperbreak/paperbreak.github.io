---
title:  "Breaking the Softmax Bottleneck"
author: "C.H."
date:   2025-02-18
categories: [theory, language modeling]
---

A breakdown of the theoretical portion of the paper [Breaking the Softmax Bottleneck: A High-Rank RNN Language Model](https://arxiv.org/abs/1711.03953)

# Background

When doing Language Modeling, we try and train a model to learn the conditional next-token distribution $\mathbb{P}(X \vert C)$, where $X= \lbrace x_1, \dots , x_m \rbrace$ is our vocabulary (set of tokens) and $C = \lbrace c_1, \dots, c_n \rbrace$ is the set of contexts. What the set $C$ looks like is based on the type of language modeling we want to perform, for example left-to-right or masked language modeling. In general, the context is just the tokens surrounding a particular token.

We can visualize this distribution as a matrix, with the number of rows being equal to the number of unique tokens $m$, and the number of columns being equal to the number of unique contexts $n$. Each entry at position $(i, j)$ represents the probability of token $x_i$ appearing in context $c_j$.

We train our NN to find weights that get as close as possible to the theoretical distribution:

$$
\begin{aligned}
\log \mathbb{P}(X \vert C) & \approx \log \mathbb{P}_\theta(X \vert C) \\
& = \log \text{Softmax} ( H_\theta W_\theta^\top ) \\
\end{aligned}
$$

Where $W_\theta$ is our embedding weights, and $H_\theta$ is our context vectors (output of the final layer of our NN *before* the output embedding).
Essentially, this boils down to a matrix factorization problem - we are trying to approximate the theoretical distribution on the left as a product of two matricies on the right.

Looking deeper, when we initialize the embedding weights $W_\theta$, we have to also pick their dimension size $d$. 
Therefore $W_\theta \in \mathbb{R}^{m \times d}$ and $H_\theta \in \mathbb{R}^{n \times d}$. This means we are performing a 
[Rank Factorization](https://en.wikipedia.org/wiki/Rank_factorization) of $\mathbb{P}(X \vert C)$, and $\text{rank}(\mathbb{P}(X \vert C)) = d$.

But this is now a bit concerning - in practice, we usually set $d$ to be a relatively low number, for example 786 for BERT.
This begins to imply the existance of the **Softmax Bottleneck**: if we set our embedding dimension $d$ too low, we will not
be able to accurately approximate the true distribution $\mathbb{P}(X \vert C)$.

# Formal Justification

## Definitions

1) Let $A = \log \mathbb{P}^*(X \vert C)$

Note: here we use the notation $\mathbb{P}^*$ to denote the true distribution. In practice, we don't know what the true distribution is - we are trying to solve for it.

2) Let $F(A)$ be the set of *row-wise* shifts of the matrix $A$:

$$
\begin{align}
F(A) & = \{ A + C \space \vert \space C_{i,:} = c, c \in \mathbb{R}  \} & (1) \\
& = \{ A + \Lambda J_{m,n} \space \vert \space \Lambda \text{ is diagonal}, \Lambda \in \mathbb{R}^{n \times n} \} & (2)
\end{align}
$$

At first glance, the set $F(A)$ might seem very unnatural - but it essentially just represents the idea that Softmax is shift-invariant:
$$
\text{Softmax}(\vec x) = \frac{\exp(\vec x)}{\sum_j \exp(x_j)}
$$
If we shift each element of $\vec x$ by the same value $c$, formally $\vec x + \vec c$ where $\vec c = [c, c, \dots, c]$, then $\text{Softmax}(\vec x + \vec c) = \text{Softmax}(\vec x)$.
$F(A)$ is just the matrix form of this idea.

Note: I wrote two equivalent definitions for $A$ for (1) and (2) - definition (1) is easier to understand,
and definition (2) is more useful for one of our proofs later.

## Properties

### Property 1

For any matrix $A'$, $A' \in F(A) \iff \text{Softmax}(A') = \mathbb{P}^*(X \vert C)$

The set $F(A)$ defines the set of all logits that correspond to the true probability distribution $\mathbb{P}^*$.  This means our logits $H_\theta W_\theta^\top$ have to be in the set $F(A)$ for our NN to be able to approximate the true distribution $\mathbb{P}^*$.

$\implies$

$$
\begin{aligned}
& & A' &\in F(A) & (1) \\
\implies & & A' &= A + C & (2) \\
\implies & & \text{Softmax}(A') &= \text{Softmax}(A + C) & (3) \\
& & &= \text{Softmax}(A) & (4) \\
& & &= \text{Softmax}(\log \mathbb{P}^*(X \vert C)) & (5) \\
& & &= \mathbb{P}^*(X \vert C) \quad \blacksquare & (6)
\end{aligned}
$$

Note: Softmax is shift-invariant (line 3)

$\impliedby$

$$
\begin{aligned}
& & \text{Softmax}(A') &= \mathbb{P}^*(X \vert C) & (1) \\
\implies & & \log \text{Softmax}(A') &= \log \mathbb{P}^*(X \vert C) & (2) \\
\implies & & \log \text{Softmax}(A') &= A & (3) \\
\implies & & A' + \log \Sigma \exp(A') &= A & (4) \\
\implies & & A' &= A - \log \Sigma \exp(A') & (5) \\
& & & \in \space F(A)  \quad \blacksquare & (6)
\end{aligned}
$$

Note: Line (4) above is abuse of notation, specifically the $\Sigma \exp(A')$. This sum is applied row-wise.

### Lemma 1

It follows immediately from property 1 that given model parameters $\theta$

$$
H_\theta W_\theta^\top \in F(A) \iff \mathbb{P}_\theta(X \vert C) = \mathbb{P}^*(X \vert C)
$$

### Property 2

$\forall A_1 \neq A_2 \in F(A), \vert \text{rank}(A_1) - \text{rank}(A_2) \vert \leq 1$

This is saying that all matrices that belong to the set $F(A)$ have a similar rank. This is not surprising - every matrix in $F(A)$ is simply *row-wise* shifted by some constant factor.

$\implies$

$$
\begin{aligned}
& & \text{Given } A_1 \neq A_2 &\in F(A) & (1) \\
& & A_1 &= A + \Lambda_1 \mathbb{J}_{m,n} & (2) \\
& & A_2 &= A + \Lambda_2 \mathbb{J}_{m,n} & (3) \\
\implies & & A_1 &= A_2 + (\Lambda_1 - \Lambda_2)\mathbb{J}_{m,n} & (4) \\
\implies & & \text{rank}(A_1) &= \text{rank}(A_2 + (\Lambda_1 - \Lambda_2)\mathbb{J}_{m,n}) & (5) \\
& & &\leq \text{rank}(A_2) + \text{rank}((\Lambda_1 - \Lambda_2)\mathbb{J}_{m,n}) & (6) \\
& & &\leq \text{rank}(A_2) + \min(\text{rank}(\Lambda_1 - \Lambda_2),\text{rank}(\mathbb{J}_{m,n})) & (7) \\
& & &\leq \text{rank}(A_2) + 1 & (8) \\
& & \text{similarly, we can find that} & & \\
& & \text{rank}(A_2) &\leq \text{rank}(A_1) - 1 & (9) \\
\implies & & \vert \text{rank}(A_1) - \text{rank}(A_2) \vert &\leq 1 \quad \blacksquare & (10)
\end{aligned}
$$

## The Softmax Bottleneck

We can now combine the properties above to prove the existance of the softmax bottleneck:

### Proposition 1

$$
\exists \space \theta \ni \mathbb{P}_\theta(X|C) = \mathbb{P}^*(X|C) \iff d \geq \min_{A' \in F(A)}\text{rank}(A')
$$

$\implies$

$$
\begin{aligned}
& & \exists \space \theta \ni \mathbb{P}_\theta(X|C) &= \mathbb{P}^*(X|C) & (1) \\
\underset{\tiny\text{Lemma 1}}{\implies} & & H_\theta W_\theta^\top &\in F(A) & (2) \\
\implies & & \exists A' \in F(A) \ni H_\theta W_\theta^\top &= A' & (3) \\
\implies & & \text{rank}(H_\theta W_\theta^\top) &= \text{rank}(A') & (4) \\
\implies & & d &= \text{rank}(A') & (5) \\
& & &\geq \min_{A'' \in F(A)} \text{rank}(A'') \quad \blacksquare & (6)
\end{aligned}
$$

$\impliedby$

$$
\begin{aligned}
& & d \geq \min_{A'' \in F(A)} &\text{rank}(A'') & & (1) \\
&\implies & \exists A' &\in F(A) & & (2a) \\
& & \exists H' &\in \mathbb{R}^{n \times d} & & (2b) \\
& & \exists W' &\in \mathbb{R}^{m \times d} & & (2c) \\
& & A' &= H'W' & & (2d) \\
&\underset{\tiny\text{Univ. A.T.}}{\implies} & \exists \space \theta \ni H' &= H_\theta & & (3a) \\
& & W' &= W_\theta & & (3b) \\
&\underset{\tiny\text{Lemma 1}}{\implies} & \mathbb{P}_\theta(X|C) &= \mathbb{P}^*(X|C) & & (4)
\end{aligned}
$$

### Corollary 1

The **Softmax Bottleneck**:

$$
d < \text{rank}(A) - 1 \implies \forall \theta \space  \exists c \in C \ni \mathbb{P}_\theta(X|c) \neq \mathbb{P}^*(X|c)
$$

$\implies$

$$
\begin{aligned}
& & d &< \text{rank}(A) - 1 & & (1) \\
&\underset{\tiny\text{Property 2}}{\implies} & H_\theta W_\theta^\top &\notin F(A) & & (2) \\
&\underset{\tiny\text{Proposition 1}}{\implies} & \forall \theta \space \exists c \in C \ni \mathbb{P}_\theta(X|c) &\neq \mathbb{P}^*(X|c) & & (3)
\end{aligned}
$$
